{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, \\\n",
    "cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, \\\n",
    "BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, \\\n",
    "precision_score, confusion_matrix, classification_report, roc_curve, auc, \\\n",
    "average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, \\\n",
    "MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from xgboost import XGBClassifier\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_test_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function to perform train_test_split and necessary preprocessing / scaling\n",
    "'''\n",
    "\n",
    "def train_test_preprocess(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    # check that there are the same number of rows in X as values in y\n",
    "    assert X_train.shape[0] == y_train.shape[0]\n",
    "    \n",
    "    # Categorizing features in preparation for scaling / encoding\n",
    "    X_train_cat_eng = X_train.select_dtypes(include=['int64']).reset_index(drop=True)\n",
    "    X_test_cat_eng = X_test.select_dtypes(include=['int64']).reset_index(drop=True)\n",
    "\n",
    "    X_train_cont = X_train.select_dtypes(exclude=['object','int64']).reset_index(drop=True)\n",
    "    X_test_cont = X_test.select_dtypes(exclude=['object','int64']).reset_index(drop=True)\n",
    "\n",
    "    cat_columns = ['market', 'region']\n",
    "    cat_train = X_train[cat_columns].reset_index(drop=True)\n",
    "    cat_test = X_test[cat_columns].reset_index(drop=True)\n",
    "\n",
    "    # Scale continuous variables using Min Max Scaler:\n",
    "    scaler = MinMaxScaler() # instantiate MinMaxScaler\n",
    "\n",
    "    ## TRAIN\n",
    "    # Fit and transform X_train\n",
    "    X_train_cont_scaled = scaler.fit_transform(X_train_cont)\n",
    "    X_train_cont_scaled = pd.DataFrame(X_train_cont_scaled, columns=X_train_cont.columns)\n",
    "\n",
    "    # One hot encode categoricals\n",
    "    ohe = OneHotEncoder(handle_unknown = 'ignore')\n",
    "    encoded_train = ohe.fit_transform(cat_train).toarray()\n",
    "    X_train_cat = pd.DataFrame(encoded_train, columns=ohe.get_feature_names(cat_train.columns))\n",
    "\n",
    "    # Putting it all together:\n",
    "    X_train_processed = pd.concat([X_train_cat, X_train_cont, X_train_cat_eng], axis=1)\n",
    "    X_train_scaled = pd.concat([X_train_cat, X_train_cont_scaled, X_train_cat_eng], axis=1) \n",
    "\n",
    "    ## TEST\n",
    "    # Scale continuous features\n",
    "    X_test_cont_scaled = scaler.transform(X_test_cont)\n",
    "    X_test_cont_scaled = pd.DataFrame(X_test_cont_scaled, columns=X_test_cont.columns)\n",
    "\n",
    "    # One hot encoding categoricals\n",
    "    encoded_test = ohe.transform(cat_test).toarray()\n",
    "    X_test_cat = pd.DataFrame(encoded_test, columns=ohe.get_feature_names(cat_test.columns))\n",
    "\n",
    "    # Putting it all together\n",
    "    X_test_scaled = pd.concat([X_test_cat, X_test_cont_scaled, X_test_cat_eng], axis=1)\n",
    "    X_test_processed = pd.concat([X_test_cat, X_test_cont, X_test_cat_eng], axis=1)\n",
    "    \n",
    "    return X_train_processed, X_train_scaled, X_test_processed, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function to print relevant scoring metrics\n",
    "'''\n",
    "\n",
    "def print_scores(y_train, y_hat_train, y_test, y_hat_test, binary=True):\n",
    "    if binary:\n",
    "        print('Training Recall: ', \n",
    "              recall_score(y_train, y_hat_train))\n",
    "        print('Testing Recall: ', \n",
    "              recall_score(y_test, y_hat_test))\n",
    "        print('\\n')\n",
    "        print('Training F1: ', \n",
    "              f1_score(y_train, y_hat_train))\n",
    "        print('Testing F1: ', \n",
    "              f1_score(y_test, y_hat_test))\n",
    "        print('\\n')\n",
    "        false_positive_rate, true_positive_rate, thresholds = \\\n",
    "        roc_curve(y_test, y_hat_test)\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "        print('ROC AUC: ', roc_auc)\n",
    "        print('PR AUC: ', average_precision_score(y_test, y_hat_test))\n",
    "        print('\\n')\n",
    "        \n",
    "    else:\n",
    "        print('Training Recall (weighted avg): ', \n",
    "              recall_score(y_train, y_hat_train, average='weighted'))\n",
    "        print('Testing Recall (weighted avg): ', \n",
    "              recall_score(y_test, y_hat_test, average='weighted'))\n",
    "        print('\\n')\n",
    "        print('Training Recall (macro avg): ', \n",
    "              recall_score(y_train, y_hat_train, average='macro'))\n",
    "        print('Testing Recall (macro avg): ', \n",
    "              recall_score(y_test, y_hat_test, average='macro'))\n",
    "        print('\\n')\n",
    "        print('Training F1-Score (weighted avg): ', \n",
    "              f1_score(y_train, y_hat_train, average='weighted'))\n",
    "        print('Testing F1-Score (weighted avg): ', \n",
    "              f1_score(y_test, y_hat_test, average='weighted'))\n",
    "        print('\\n')\n",
    "        print('Training F1-Score (macro avg): ', \n",
    "              f1_score(y_train, y_hat_train, average='macro'))\n",
    "        print('Testing F1-Score (macro avg): ', \n",
    "              f1_score(y_test, y_hat_test, average='macro'))\n",
    "        print('\\n')\n",
    "        print('Testing Recall (failure class): ', \n",
    "              recall_score(y_test, y_hat_test, average=None, labels=[1]))\n",
    "        print('\\n')\n",
    "    \n",
    "    print('Training Accuracy: ', accuracy_score(y_train, y_hat_train))\n",
    "    print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### return_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function that stores relevant scoring metrics\n",
    "'''\n",
    "\n",
    "def return_scores(y_train, y_hat_train, y_test, y_hat_test):\n",
    "    r_train = recall_score(y_train, y_hat_train)\n",
    "    r_test = recall_score(y_test, y_hat_test)\n",
    "         \n",
    "    f1_train = f1_score(y_train, y_hat_train)\n",
    "    f1_test = f1_score(y_test, y_hat_test)\n",
    "\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_hat_test)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    pr_auc = average_precision_score(y_test, y_hat_test)\n",
    "    \n",
    "    ac_train = accuracy_score(y_train, y_hat_train)\n",
    "    ac_test = accuracy_score(y_test, y_hat_test)\n",
    "    \n",
    "    return r_train, r_test, f1_train, f1_test, ac_train, ac_test, roc_auc, pr_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_test_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function that checks new train & test splits for proper shape\n",
    "'''\n",
    "\n",
    "def train_test_check(X_train_processed, X_train_scaled, X_test_processed, \n",
    "                     X_test_scaled, y_train, y_test):\n",
    "    \n",
    "    assert X_train_processed.shape[0] == y_train.shape[0]\n",
    "    assert X_train_scaled.shape[0] == y_train.shape[0]\n",
    "    \n",
    "    assert X_test_processed.shape[0] == y_test.shape[0]\n",
    "    assert X_test_scaled.shape[0] == y_test.shape[0]\n",
    "\n",
    "    print(\"There are {} features in train set\".format(len(X_train_processed.columns)))\n",
    "    print(\"There are {} features in test set\".format(len(X_test_processed.columns)))\n",
    "    print('\\n')\n",
    "\n",
    "    print(\"There are {} features in train set (scaled)\".format(len(X_train_scaled.columns)))\n",
    "    print(\"There are {} features in test set (scaled)\".format(len(X_test_scaled.columns)))\n",
    "    print('\\n')\n",
    "    \n",
    "    print(f\"y_train is a Series with {y_train.shape[0]} values\")\n",
    "    print('\\n')\n",
    "    print(\"target breakdown: \", y_train.value_counts(normalize=True))\n",
    "\n",
    "    display(X_train_processed.head())\n",
    "    display(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correlation_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Function that checks for excessive correlations across features\n",
    "'''\n",
    "def correlation_check(X_train_processed):\n",
    "    \n",
    "    df_corr=X_train_processed.corr()\n",
    "\n",
    "    df = df_corr.abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "    df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "    df.set_index(['pairs'], inplace = True)\n",
    "    df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "    # cc for correlation coefficient\n",
    "    df.columns = ['cc']\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    display(df[(df.cc>.5) & (df.cc<1)])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
